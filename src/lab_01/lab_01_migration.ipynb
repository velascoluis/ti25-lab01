{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrating from a legacy Hadoop enviroment to Google Data Cloud\n",
    "\n",
    "This notebook will guide you through the process of migrating from a legacy Hadoop enviroment to Google Data Cloud.\n",
    "It contains the following steps:\n",
    "1. Deploying a legacy Cloudera cluster.\n",
    "2. Migrate data from the legacy Hadoop enviroment to BigQuery.\n",
    "3. Migrate HIVE workloads to BigQuery\n",
    "4. Migrate PySpark workloads to BigQuery Spark.\n",
    "5. Enable BigQuery Data Governance (data quality, data profiling and data insights.\n",
    "6. Run an BigQuery Canvas analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable the following org policies:\n",
    "- `constraints/compute.vmExternalIpAccess`\n",
    "- `constraints/compute.requireShieldedVm`\n",
    "- `constraints/iam.disableServiceAccountKeyCreation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Deploying a legacy Cloudera cluster on Google Cloud.\n",
    "\n",
    "From a Cloud Shell, clone the TI25-LAB01 repository and run the deploy-cloudera.sh script specifying the project id, region and zone.\n",
    "For example:\n",
    "```bash\n",
    "git clone https://github.com/velascoluis/ti25-lab01.git\n",
    "cd ti25-lab01/src/lab_01/cdh-deployment\n",
    "./deploy-cloudera.sh velascoluis-dev-sandbox us-central1 us-central1-a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once deployed connect to the Cloudera container, for example:\n",
    "```bash\n",
    "gcloud compute ssh gce-cdh-5-single-node --zone=us-central1-a --container 39334b3d7c28\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$> hive\n",
    "hive> use ccf_db;\n",
    "hive> show tables;\n",
    "hive> select * from customers;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run a more complex HIVE query:\n",
    "\n",
    "```sql\n",
    "hive> WITH CustomerLoanSummary AS (\n",
    "    -- Summarize loan applications per customer\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        c.life_event,\n",
    "        COUNT(la.application_id) as total_applications,\n",
    "        SUM(CASE WHEN la.application_status = 'APPROVED' THEN 1 ELSE 0 END) as approved_applications,\n",
    "        SUM(la.loan_amount) as total_loan_amount,\n",
    "        SUM(la.marketing_cost) as total_marketing_cost,\n",
    "        AVG(la.loan_amount) as avg_loan_amount\n",
    "    FROM customers c\n",
    "    LEFT JOIN loan_applications la ON c.customer_id = la.customer_id\n",
    "    GROUP BY c.customer_id, c.first_name, c.last_name, c.life_event\n",
    "),\n",
    "RepaymentMetrics AS (\n",
    "    -- Calculate repayment performance metrics\n",
    "    SELECT \n",
    "        la.customer_id,\n",
    "        COUNT(DISTINCT lr.repayment_id) as total_repayments,\n",
    "        SUM(lr.amount_paid) as total_amount_paid,\n",
    "        AVG(lr.days_past_due) as avg_days_past_due,\n",
    "        SUM(CASE WHEN lr.payment_status = 'LATE' THEN 1 ELSE 0 END) as late_payments\n",
    "    FROM loan_applications la\n",
    "    JOIN loan_repayments lr ON la.application_id = lr.loan_id\n",
    "    GROUP BY la.customer_id\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    cls.*,\n",
    "    rm.total_repayments,\n",
    "    rm.total_amount_paid,\n",
    "    rm.avg_days_past_due,\n",
    "    rm.late_payments,\n",
    "    ROUND(CASE \n",
    "        WHEN cls.total_loan_amount = 0 THEN NULL \n",
    "        ELSE (cls.total_marketing_cost / cls.total_loan_amount) * 100 \n",
    "    END, 2) as marketing_cost_percentage,\n",
    "    ROUND(CASE \n",
    "        WHEN rm.total_repayments = 0 THEN NULL \n",
    "        ELSE (rm.late_payments / rm.total_repayments) * 100 \n",
    "    END, 2) as late_payment_percentage,\n",
    "    CASE \n",
    "        WHEN rm.avg_days_past_due = 0 THEN 'Excellent'\n",
    "        WHEN rm.avg_days_past_due <= 30 THEN 'Good'\n",
    "        WHEN rm.avg_days_past_due <= 90 THEN 'Fair'\n",
    "        ELSE 'Poor'\n",
    "    END as customer_rating\n",
    "FROM CustomerLoanSummary cls\n",
    "LEFT JOIN RepaymentMetrics rm ON cls.customer_id = rm.customer_id\n",
    "WHERE cls.total_applications > 0\n",
    "ORDER BY cls.total_loan_amount DESC, rm.avg_days_past_due ASC;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HIVE query](./assets/hive.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the PySpark code to calculate the delinquency rate:\n",
    "```bash\n",
    "cd /home/cloudera\n",
    "spark-submit del_rate_calculation.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PySpark code](./assets/spark.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Migrate data from the legacy Hadoop enviroment to BigQuery.\n",
    "\n",
    "Still from the Cloudera container, run the following commands, adapt the bucket name to your project.\n",
    "First, ensure you have vibility to the Google Cloud Storage bucket:\n",
    "```bash\n",
    "export BUCKET_NAME=velascoluis-dev-sandbox-cloudera-1737643339\n",
    "hadoop fs -ls  gs://${BUCKET_NAME}\n",
    "```\n",
    "Then copy the data from the Hadoop enviroment to the BigQuery bucket:\n",
    "\n",
    "```bash\n",
    "export BUCKET_NAME=velascoluis-dev-sandbox-cloudera-1737643339\n",
    "for TABLE_NAME in customers loan_applications loan_repayments; do\n",
    "  echo \"Processing table: ${TABLE_NAME}\"\n",
    "  FILES_LOCATION=`hive --database ${DATABASE_NAME} -S -e \"describe formatted ${TABLE_NAME} ;\" | grep 'Location' | awk '{ print $NF }'`\n",
    "  echo \"Source location: ${FILES_LOCATION}\"\n",
    "  echo \"Copying to GCS bucket: gs://${BUCKET_NAME}/${TABLE_NAME}/${TABLE_NAME}\"\n",
    "  hadoop distcp -overwrite -delete ${FILES_LOCATION}/* gs://${BUCKET_NAME}/${TABLE_NAME}/${TABLE_NAME}\n",
    "done\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the BigQuery tables from BigQuery Studio:\n",
    "```sql\n",
    "CREATE SCHEMA IF NOT EXISTS ccf_db;\n",
    "--BigLake table\n",
    "CREATE OR REPLACE EXTERNAL TABLE `ccf_db.customers`\n",
    " WITH CONNECTION `projects/velascoluis-dev-sandbox/locations/us/connections/biglake-connection`\n",
    " OPTIONS (\n",
    "    format =\"PARQUET\",\n",
    "    uris = ['gs://velascoluis-dev-sandbox-cloudera-1737643339/customers/*']);\n",
    "\n",
    "CREATE OR REPLACE EXTERNAL TABLE `ccf_db.loan_applications`\n",
    " WITH CONNECTION `projects/velascoluis-dev-sandbox/locations/us/connections/biglake-connection`\n",
    " OPTIONS (\n",
    "    format =\"PARQUET\",\n",
    "    uris = ['gs://velascoluis-dev-sandbox-cloudera-1737643339/loan_applications/*']);    \n",
    "\n",
    "CREATE OR REPLACE EXTERNAL TABLE `ccf_db.loan_repayments`\n",
    " WITH CONNECTION `projects/velascoluis-dev-sandbox/locations/us/connections/biglake-connection`\n",
    " OPTIONS (\n",
    "    format =\"PARQUET\",\n",
    "    uris = ['gs://velascoluis-dev-sandbox-cloudera-1737643339/loan_repayments/*']);\n",
    "\n",
    "\n",
    "CREATE OR REPLACE TABLE `ccf_db.customers_iceberg`\n",
    "(\n",
    "  customer_id STRING,\t\n",
    "  first_name STRING,\t\n",
    "  last_name STRING,\n",
    "  date_of_birth STRING,\n",
    "  email STRING,\n",
    "  phone STRING,\n",
    "  registration_date STRING,\n",
    "  life_event STRING\n",
    ")\n",
    " WITH CONNECTION `projects/velascoluis-dev-sandbox/locations/us/connections/biglake-connection`\n",
    " OPTIONS (\n",
    "    file_format = 'PARQUET',\n",
    "    table_format = 'ICEBERG',\n",
    "    storage_uri = 'gs://velascoluis-dev-sandbox-cloudera-1737643339/warehouse/customers/');\n",
    "\n",
    "INSERT INTO `ccf_db.customers_iceberg`\n",
    "SELECT * FROM `ccf_db.customers`;\n",
    "\n",
    "\n",
    "CREATE OR REPLACE TABLE `ccf_db.loan_applications_iceberg`\n",
    "(\n",
    "  application_id STRING,\t\n",
    "  customer_id STRING,\t\n",
    "  application_date STRING,\n",
    "  product_type STRING,\n",
    "  loan_amount NUMERIC,\n",
    "  application_status STRING,\n",
    "  application_channel STRING,\n",
    "  marketing_cost NUMERIC,\n",
    "  approval_date STRING,\n",
    "  disbursement_date STRING\n",
    ")\n",
    " WITH CONNECTION `projects/velascoluis-dev-sandbox/locations/us/connections/biglake-connection`\n",
    " OPTIONS (\n",
    "    file_format = 'PARQUET',\n",
    "    table_format = 'ICEBERG',\n",
    "    storage_uri = 'gs://velascoluis-dev-sandbox-cloudera-1737643339/warehouse/loan_applications/');\n",
    "\n",
    "INSERT INTO `ccf_db.loan_applications_iceberg`\n",
    "SELECT * FROM `ccf_db.loan_applications`;\n",
    "\n",
    "CREATE OR REPLACE TABLE `ccf_db.loan_repayments_iceberg`\n",
    "(\n",
    "  repayment_id STRING,\n",
    "  loan_id STRING,\n",
    "  repayment_date STRING,\n",
    "  amount_due NUMERIC,\n",
    "  amount_paid NUMERIC,\n",
    "  payment_status STRING,\n",
    "  days_past_due INTEGER\n",
    ")\n",
    " WITH CONNECTION `projects/velascoluis-dev-sandbox/locations/us/connections/biglake-connection`\n",
    " OPTIONS (\n",
    "    file_format = 'PARQUET',\n",
    "    table_format = 'ICEBERG',\n",
    "    storage_uri = 'gs://velascoluis-dev-sandbox-cloudera-1737643339/warehouse/loan_repayments/');\n",
    "\n",
    "INSERT INTO `ccf_db.loan_repayments_iceberg`\n",
    "SELECT * FROM `ccf_db.loan_repayments`;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Migrate HIVE workloads to BigQuery\n",
    "\n",
    "- In BigQuery enable SQL translation and translate from HQL to Google SQL.\n",
    "- Then explain the change with Generative AI.\n",
    "- Adapt the table names to BigQuery adding the dataset name using Generative AI.\n",
    "- Materialize the results on a BigQuery table adding: `CREATE OR REPLACE TABLE ccf_db.hive_analysis AS ..` \n",
    "\n",
    "![HIVE query](./assets/hive.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Migrate PySpark workloads to BigQuery Spark.\n",
    "\n",
    "From the BigQuery Studio, create a new python notebook and run the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor pySpark code as:\n",
    "```python\n",
    "from dataproc_spark_session.session.spark.connect import DataprocSparkSession\n",
    "from google.cloud.dataproc_v1 import Session\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "project = \"velascoluis-dev-sandbox\" # @param {type:\"string\"}\n",
    "location = \"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "session = Session()\n",
    "spark = (\n",
    "    DataprocSparkSession.builder\n",
    "      .appName(\"sparkSession\")\n",
    "      .remote()\n",
    "      .dataprocConfig(session)\n",
    "      .getOrCreate()\n",
    ")\n",
    "# Load data from BigQuery tables\n",
    "loan_apps_df = spark.read.format(\"bigquery\").option(\"table\", \"ccf_db.loan_applications\").load()\n",
    "repayments_df = spark.read.format(\"bigquery\").option(\"table\", \"ccf_db.loan_repayments\").load()\n",
    "customers_df = spark.read.format(\"bigquery\").option(\"table\", \"ccf_db.customers\").load()\n",
    "\n",
    "\n",
    "print(\"\\nNumber of customers: \" + str(customers_df.count()))\n",
    "print(\"Number of loan applications: \" + str(loan_apps_df.count()))\n",
    "print(\"Number of repayments: \" + str(repayments_df.count()) + \"\\n\")\n",
    "print(\"Schema for loan_apps_df:\")\n",
    "loan_apps_df.printSchema()\n",
    "print()\n",
    "print(\"Schema for repayments_df:\")\n",
    "repayments_df.printSchema()\n",
    "print()\n",
    "print(\"Schema for customers_df:\")\n",
    "customers_df.printSchema()\n",
    "\n",
    "\n",
    "approved_loans = loan_apps_df.filter(\"application_status = 'Approved'\")\n",
    "\n",
    "\n",
    "repayments_alias = repayments_df.alias(\"repayments\")\n",
    "approved_loans_alias = approved_loans.alias(\"approved_loans\")\n",
    "customers_alias = customers_df.alias(\"customers\")\n",
    "\n",
    "\n",
    "loan_repayments = (\n",
    "    repayments_alias\n",
    "    .join(\n",
    "        approved_loans_alias,\n",
    "        repayments_alias.loan_id == approved_loans_alias.application_id,\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .join(\n",
    "        customers_alias,\n",
    "        approved_loans_alias.customer_id == customers_alias.customer_id,\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"repayments.repayment_id\"),\n",
    "        F.col(\"repayments.loan_id\"),\n",
    "        F.col(\"repayments.repayment_date\"),\n",
    "        F.col(\"repayments.amount_due\"),\n",
    "        F.col(\"repayments.amount_paid\"),\n",
    "        F.col(\"repayments.payment_status\"),\n",
    "        F.col(\"repayments.days_past_due\"),\n",
    "        F.col(\"approved_loans.product_type\"),\n",
    "        F.col(\"customers.first_name\"),\n",
    "        F.col(\"customers.last_name\"),\n",
    "        F.col(\"customers.customer_id\").alias(\"customer_id_customers\")\n",
    "    )\n",
    ")\n",
    "\n",
    "delinquent_loans = loan_repayments.withColumn(\n",
    "    \"delinquency_bucket\",\n",
    "    F.when(F.col(\"days_past_due\") == 0, \"Current\")\n",
    "    .when(F.col(\"days_past_due\").between(1, 29), \"1-29 Days\")\n",
    "    .when(F.col(\"days_past_due\").between(30, 59), \"30-59 Days\")\n",
    "    .when(F.col(\"days_past_due\").between(60, 89), \"60-89 Days\")\n",
    "    .when(F.col(\"days_past_due\") >= 90, \"90+ Days\")\n",
    "    .otherwise(\"Current\")\n",
    ")\n",
    "\n",
    "product_totals = delinquent_loans.groupBy(\"product_type\").agg(\n",
    "    F.count(\"*\").alias(\"total_payments\")\n",
    ")\n",
    "\n",
    "delinquency_counts = delinquent_loans.groupBy(\n",
    "    \"product_type\", \"delinquency_bucket\"\n",
    ").agg(\n",
    "    F.count(\"*\").alias(\"count\")\n",
    ")\n",
    "\n",
    "delinquency_rates = delinquency_counts.join(\n",
    "    product_totals, \"product_type\"\n",
    ").withColumn(\n",
    "    \"delinquency_rate\",\n",
    "    F.round((F.col(\"count\") / F.col(\"total_payments\")) * 100, 2)\n",
    ")\n",
    "\n",
    "window_spec = Window.partitionBy(\"product_type\", \"delinquency_bucket\").orderBy(\n",
    "    F.desc(\"days_past_due\")\n",
    ")\n",
    "\n",
    "top_delinquents = (\n",
    "    delinquent_loans\n",
    "    .filter(F.col(\"days_past_due\") > 0)\n",
    "    .select(\n",
    "        \"product_type\",\n",
    "        \"delinquency_bucket\",\n",
    "        \"days_past_due\",\n",
    "        \"first_name\",\n",
    "        \"last_name\",\n",
    "        \"customer_id_customers\"\n",
    "    )\n",
    "    .withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "    .filter(F.col(\"rank\") <= 3)\n",
    "    .withColumn(\n",
    "        \"customer_name\",\n",
    "        F.concat(F.col(\"first_name\"), F.lit(\" \"), F.col(\"last_name\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "results = delinquency_rates.orderBy(\"product_type\", \"delinquency_bucket\").collect()\n",
    "\n",
    "csv_data = []\n",
    "\n",
    "for row in results:\n",
    "    base_row = {\n",
    "        \"product_type\": row.product_type,\n",
    "        \"delinquency_bucket\": row.delinquency_bucket,\n",
    "        \"delinquency_rate\": row.delinquency_rate,\n",
    "    }\n",
    "    if row.delinquency_bucket != \"Current\":\n",
    "        matching_customers = top_delinquents.filter(\n",
    "            (F.col(\"product_type\") == row.product_type) &\n",
    "            (F.col(\"delinquency_bucket\") == row.delinquency_bucket)\n",
    "        ).collect()\n",
    "        for i, cust in enumerate(matching_customers, 1):\n",
    "            cust_num = f\"customer_{i}_\"\n",
    "            base_row[cust_num + \"name\"] = cust.customer_name\n",
    "            base_row[cust_num + \"id\"] = cust.customer_id_customers\n",
    "            base_row[cust_num + \"days_past_due\"] = cust.days_past_due\n",
    "    csv_data.append(base_row)\n",
    "\n",
    "print(\"\\nDelinquency Rates by Product Type:\")\n",
    "print(\"===================================\")\n",
    "for row in csv_data:\n",
    "    print(row)\n",
    "delinquency_rates.write.format(\"bigquery\").option(\"writeMethod\", \"direct\").mode(\"overwrite\").save('ccf_db.deliquency_analysis')    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Enable BigQuery Data Governance\n",
    "\n",
    "From the BigQuery Studio, navigate to the table `ccf_db.deliquency_analysis` and enable the following data governance features:\n",
    "\n",
    "- Data profile\n",
    "- Data quality\n",
    "- Metadata curation\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
